{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import torchvision\n",
    "import torchvision.datasets as dset\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader,Dataset\n",
    "import matplotlib.pyplot as plt\n",
    "import torchvision.utils\n",
    "import numpy as np\n",
    "import random\n",
    "import os\n",
    "from PIL import Image\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "import PIL.ImageOps    \n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "from prettytable import PrettyTable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def imshow(img,text=None,should_save=False):\n",
    "    npimg = img.numpy()\n",
    "    plt.axis(\"off\")\n",
    "    if text:\n",
    "        plt.text(75, 8, text, style='italic',fontweight='bold',\n",
    "            bbox={'facecolor':'white', 'alpha':0.8, 'pad':10})\n",
    "    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
    "    plt.show()    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SiameseDataset(Dataset):\n",
    "    def __init__(self,root_dir,imageFolderDataset, transform=None, should_invert=True):\n",
    "        self.root_dir=root_dir\n",
    "        self.imageFolderDataset=imageFolderDataset\n",
    "        self.transform=transform\n",
    "        self.should_invert=should_invert\n",
    "        \n",
    "    def __getitem__(self,index):\n",
    "        should_get_same_class=random.randint(0,1)\n",
    "        #print(should_get_same_class)\n",
    "        img1_tuple=random.choice(self.imageFolderDataset.imgs)\n",
    "        if should_get_same_class:\n",
    "            while True:\n",
    "                img2_tuple=random.choice(self.imageFolderDataset.imgs)\n",
    "                if img1_tuple[1]==img2_tuple[1]:\n",
    "                    break\n",
    "        else:\n",
    "            while True:\n",
    "                img2_tuple=random.choice(self.imageFolderDataset.imgs)\n",
    "                if img1_tuple[1]!=img2_tuple[1]:\n",
    "                    break\n",
    "        \n",
    "        img1=Image.open(img1_tuple[0])\n",
    "        img2=Image.open(img2_tuple[0])\n",
    "        \n",
    "        if self.should_invert:\n",
    "            img1=PIL.ImageOps.invert(img1)\n",
    "            img2=PIL.ImageOps.invert(img2)\n",
    "        \n",
    "        if self.transform is not None:\n",
    "            img1=self.transform(img1)\n",
    "            img2=self.transform(img2)\n",
    "            \n",
    "        return img1, img2, torch.from_numpy(np.array([(img1_tuple[1]!=img2_tuple[1])], dtype=np.float32))\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.imageFolderDataset.imgs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_size=105 \n",
    "epochs=25\n",
    "batchsize=64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train_dir='./data/Omniglot/alphabet_dataset/images_background/'\n",
    "#train_dir='./data/Omniglot/character_dataset/train/'\n",
    "train_dir='./data/Omniglot/changed/train/'\n",
    "train_imagefolder=dset.ImageFolder(train_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset=SiameseDataset(root_dir=train_dir, imageFolderDataset=train_imagefolder,\n",
    "                             transform=transforms.Compose([transforms.Resize((img_size,img_size)),\n",
    "                                                           transforms.RandomHorizontalFlip(),\n",
    "                                                           transforms.RandomResizedCrop(img_size, scale=(0.8,1.0)),\n",
    "                                                           transforms.ToTensor()\n",
    "                                                          ]),\n",
    "                             should_invert=False)\n",
    "\n",
    "train_dataloader=DataLoader(train_dataset, batch_size=batchsize, num_workers=0, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_dataloader = DataLoader(train_dataset,\n",
    "                        shuffle=True,\n",
    "                        num_workers=0,\n",
    "                        batch_size=8)\n",
    "dataiter = iter(visualize_dataloader)\n",
    "\n",
    "\n",
    "example_batch = next(dataiter)\n",
    "concatenated = torch.cat((example_batch[0],example_batch[1]),0)\n",
    "imshow(torchvision.utils.make_grid(concatenated))\n",
    "print(example_batch[2].numpy()) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Koch et al.   #img_size=105\n",
    "class SiameseKoch(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SiameseKoch, self).__init__()\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(1, 64, 10),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(2),\n",
    "            nn.Conv2d(64, 128, 7),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(2),\n",
    "            nn.Conv2d(128, 128, 4),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(2),\n",
    "            nn.Conv2d(128, 256, 4),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(inplace=True),\n",
    "        )\n",
    "        \n",
    "        self.fc1=nn.Sequential(\n",
    "            nn.Linear(9216, 4096),\n",
    "            nn.Sigmoid())\n",
    "        \n",
    "        self.out=nn.Linear(4096,1)\n",
    "        \n",
    "    def forward_once(self,inp):\n",
    "        inp=self.conv(inp)\n",
    "        inp=inp.view(inp.size()[0], -1)\n",
    "        inp=self.fc1(inp)\n",
    "        return inp\n",
    "        \n",
    "    def forward(self, inp1, inp2):\n",
    "        out1=self.forward_once(inp1)\n",
    "        out2=self.forward_once(inp2)\n",
    "        #return out1,out2\n",
    "        dis=torch.abs(out2-out1)\n",
    "        out=self.out(dis)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ContrastiveLoss(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    Contrastive loss function.\n",
    "    Based on: http://yann.lecun.com/exdb/publis/pdf/hadsell-chopra-lecun-06.pdf\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, margin=2.0):\n",
    "        super(ContrastiveLoss, self).__init__()\n",
    "        self.margin = margin\n",
    "\n",
    "    def forward(self, output1, output2, label):\n",
    "        euclidean_distance = F.pairwise_distance(output1, output2)\n",
    "        loss_contrastive = torch.mean((1-label) * torch.pow(euclidean_distance, 2) +\n",
    "                                      (label) * torch.pow(torch.clamp(self.margin - euclidean_distance, min=0.0), 2))\n",
    "\n",
    "\n",
    "        return loss_contrastive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net=SiameseKoch().to(device)\n",
    "#criterion=ContrastiveLoss()\n",
    "criterion=nn.BCEWithLogitsLoss()\n",
    "optimizer=optim.Adam(net.parameters(), lr=0.001)\n",
    "#optimizer = optim.SGD(net.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_dir='./data/Omniglot/changed/valid/'\n",
    "valid_imagefolder=dset.ImageFolder(valid_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_dataset=SiameseDataset(root_dir=valid_dir, imageFolderDataset=valid_imagefolder,\n",
    "                             transform=transforms.Compose([transforms.Resize((img_size,img_size)),\n",
    "                                                           transforms.RandomHorizontalFlip(),\n",
    "                                                           transforms.RandomResizedCrop(img_size, scale=(0.8,1.0)),\n",
    "                                                           transforms.ToTensor()\n",
    "                                                          ]),\n",
    "                             should_invert=False)\n",
    "\n",
    "valid_dataloader=DataLoader(valid_dataset, batch_size=batchsize, num_workers=0, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(net, train_dataloader, valid_dataloader, epochs, criterion):\n",
    "    train_loss=[] #training loss for every epoch\n",
    "    valid_loss=[] #validation loss for every epoch\n",
    "    sum_train_loss=0.0 #sum of training losses for every epoch\n",
    "    sum_valid_loss=0.0 #sum of validation losses for every epoch\n",
    "    \n",
    "    for epoch in range(1, epochs+1):\n",
    "        train_epoch_loss=0.0\n",
    "        net.train()\n",
    "        for i, data in enumerate(train_dataloader,0):\n",
    "            img1, img2, label = data\n",
    "            img1, img2, label = img1.to(device), img2.to(device), label.to(device)\n",
    "            \n",
    "            label = label.float()\n",
    "            output = net(img1, img2)\n",
    "            loss = criterion(output, label)\n",
    "            \n",
    "            #output1, output2 = net(img1, img2)\n",
    "            #loss = criterion(output1, output2, label)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            train_epoch_loss = train_epoch_loss + ((1/(i+1)) * (loss.item() - train_epoch_loss))\n",
    "            \n",
    "        train_loss.append(train_epoch_loss)\n",
    "        sum_train_loss+=train_epoch_loss\n",
    "        \n",
    "        valid_epoch_loss=0.0\n",
    "        correct=0\n",
    "        accuracy=0\n",
    "        with torch.no_grad():\n",
    "            net.eval()\n",
    "            for i, data in enumerate(valid_dataloader,0):\n",
    "                img1, img2, label = data\n",
    "                img1, img2, label = img1.to(device), img2.to(device), label.to(device)\n",
    "                \n",
    "                output = net(img1, img2)\n",
    "                loss = criterion(output, label)\n",
    "                \n",
    "                #output1, output2 = net(img1, img2)\n",
    "                #loss = criterion(output1, output2, label)\n",
    "                                    \n",
    "                valid_epoch_loss = valid_epoch_loss + ((1/(i+1)) * (loss.item() - valid_epoch_loss))\n",
    "                \n",
    "        valid_loss.append(valid_epoch_loss)\n",
    "        sum_valid_loss+=valid_epoch_loss\n",
    "        \n",
    "        print(\"Epoch {}/{}\\n Train loss : {} \\t Valid loss {}\\n\"\n",
    "             .format(epoch, epochs, train_epoch_loss, valid_epoch_loss))\n",
    "        \n",
    "    print(\"Average training loss after {} epochs : {}\".format(epochs, sum_train_loss/epochs))\n",
    "    print(\"Average validation loss after {} epochs : {}\".format(epochs, sum_valid_loss/epochs))\n",
    "    \n",
    "    return train_loss, valid_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_losses, valid_losses = train(net, train_dataloader, valid_dataloader, epochs, criterion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.xlabel('epochs')\n",
    "plt.ylabel('loss')\n",
    "plt.plot(train_losses, label=\"Train loss\")\n",
    "plt.plot(valid_losses, label=\"Validation loss\")\n",
    "plt.legend(bbox_to_anchor=(1.1,1.0), loc='upper left')\n",
    "plt.savefig('siamese.png',dpi=300,bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dir='./data/Omniglot/changed/test/'\n",
    "test_imagefolder=dset.ImageFolder(test_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset=SiameseDataset(root_dir=test_dir, imageFolderDataset=test_imagefolder,\n",
    "                             transform=transforms.Compose([transforms.Resize((img_size,img_size)),\n",
    "                                                           transforms.RandomHorizontalFlip(),\n",
    "                                                           transforms.RandomResizedCrop(img_size, scale=(0.8,1.0)),\n",
    "                                                           transforms.ToTensor()\n",
    "                                                          ]),\n",
    "                             should_invert=False)\n",
    "\n",
    "test_dataloader=DataLoader(test_dataset, batch_size=1, num_workers=0, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval(net, test_dataloader):\n",
    "    with torch.no_grad():\n",
    "        net.eval()\n",
    "        count=100\n",
    "        correct=0\n",
    "        accuracy=0\n",
    "        dataiter = iter(test_dataloader)\n",
    "    \n",
    "        for i in range(count):\n",
    "            img1,img2,label=next(dataiter)\n",
    "            \n",
    "            cat=torch.cat((img1, img2),0)\n",
    "            output = net(Variable(img1).to(device), Variable(img2).to(device))\n",
    "            prediction = torch.sigmoid(output)\n",
    "            total = label.size(0)\n",
    "            \n",
    "            for j in range(output.size(0)):\n",
    "                if (prediction[j]>0.5) and (label[j]==1):\n",
    "                    correct+=1\n",
    "                elif (prediction[j]<0.5) and (label[j]==0):\n",
    "                    correct+=1\n",
    "            accuracy+=correct/total\n",
    "            correct=0\n",
    "            imshow(torchvision.utils.make_grid(cat),'Pred : {:.2f} Label : {}'.format(prediction.item(),label.item()))   \n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc=eval(net, test_dataloader)\n",
    "print(\"Accuracy of the network : \",acc)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env-siamese",
   "language": "python",
   "name": "env-siamese"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
